stopgrad(x) = x
Zygote.@nograd stopgrad

abstract type Quantizer end
struct VectorQuantizer <: Quantizer 
    embedding::AbstractArray{<:Real, 2}
end

struct VectorQuantizerEMA <: Quantizer 
    embedding::AbstractArray{<:Real, 2}
    n::AbstractArray{<:Real, 2}
    m::AbstractArray{<:Real, 2}
end

Flux.@functor VectorQuantizer
Flux.@functor VectorQuantizerEMA
Flux.trainable(q::Quantizer) = (q.embedding)

function (q::Quantizer)(z::AbstractArray{<:Real})
    # flatten input zâ‚‘ #d, n, bs = size(zâ‚‘)
    size_ = size(z)
    d, n_elements = size_[1], prod(size_[2:end]) # now it works for nD array

    zâ‚‘ = reshape(z, d, n_elements)
    # Compute distance (e - x)Â² = eÂ² - 2ex + xÂ²
    dist = (sum(zâ‚‘ .^2, dims=1) 
            .+ sum(q.embedding .^2, dims=1)' 
            .- (2 .* q.embedding' * zâ‚‘ ))

    # Encoding
    encoding_indices = argmin(dist, dims=1)
    encodings = zeros_like(zâ‚‘, (size(q.embedding, 2), n_elements)) # there has to be transposition or dropdims
    Zygote.ignore(()->(encodings[encoding_indices] .+= 1)) # need to exclude gradient or i will crash

    # Quantize and unflatten
    quantized = q.embedding * encodings # (d, e) * (e, bs) -> (d, bs)
    quantized = reshape(quantized, size_)
    return quantized, encodings
end

function ema_update!(q::VectorQuantizerEMA, z, encodings; Î³=0.99f0, Ïµ=1f-5)
    q.n .= q.n .* Î³ .+ (1f0 - Î³) .* sum(encodings, dims=2) # (e, 1)
    N = sum(q.n)
    q.n .= (q.n .+ Ïµ) ./ (N .+ size(q.embedding, 2) .* Ïµ) .* N # (e, 1)

    d = size(z, 1)
    dw = reshape(z, d, :) * encodings' # (d, n_elements) x (n_elements, e) -> (d, e)

    q.m .= q.m .* Î³ .+ (1f0 - Î³) .* dw # (d, e)
    q.embedding .= q.m ./ q.n' # (d, e) .* (1, e) -> (d, e)
end

struct VQVAE
    encoder
    quantizer::Quantizer
    decoder
end

Flux.@functor VQVAE
Flux.trainable(model::VQVAE) = (model.encoder, model.quantizer, model.decoder)

function (model::VQVAE)(x::AbstractArray{T}) where T<:Real
    zâ‚‘ =  model.encoder(x)
    quantized, _ = model.quantizer(zâ‚‘)
    quantized = zâ‚‘ + stopgrad(quantized - zâ‚‘) # gradient bypass
    xÌ‚ = model.decoder(quantized)  
    return xÌ‚
end


function loss_gradient(model::VQVAE, x::AbstractArray{T}; Î²::T=T(1)) where T<:Real
    zâ‚‘ =  model.encoder(x)
    quantized, _ = model.quantizer(zâ‚‘)
    
    # Embedding loss (e -> zâ‚‘) + Commitment loss (zâ‚‘ -> e)
    e_latent_loss = Flux.Losses.mse(stopgrad(quantized), zâ‚‘) # gradient propagation to encoder
    q_latent_loss = Flux.Losses.mse(quantized, stopgrad(zâ‚‘)) # gradient propagation to embedding

    ğ“›â‚—â‚â‚œâ‚‘â‚™â‚œ = q_latent_loss + Î² * e_latent_loss

    # Bypass of gradients from decoder to encoder 
    quantized = zâ‚‘ + stopgrad(quantized - zâ‚‘)
    xÌ‚ = model.decoder(quantized)  
    
    ğ“› = Flux.Losses.mse(x, xÌ‚) + ğ“›â‚—â‚â‚œâ‚‘â‚™â‚œ # in paper was mse(x, xÌ‚) / data_variance
end

function loss_ema(model::VQVAE, x::AbstractArray{T}; Î²::T=T(1), Î³::T=T(0.99), Ïµ::T=T(1e-5), trainmode::Bool=true) where T<:Real
    zâ‚‘ =  model.encoder(x)
    quantized, encodings = model.quantizer(zâ‚‘)
    # Commitment loss (zâ‚‘ -> embedding)
    e_latent_loss = Flux.Losses.mse(stopgrad(quantized), zâ‚‘) # gradient propagation to encoder
    ğ“›â‚—â‚â‚œâ‚‘â‚™â‚œ = Î² * e_latent_loss

    if trainmode 
        # Exponential Moving Average update
        Zygote.ignore(()->ema_update!(model.quantizer, zâ‚‘, encodings; Î³=Î³, Ïµ=Ïµ))
        # Zygote.ignore is probably not needed but .... just to be sure
    end

    # Bypass of gradients from decoder to encoder 
    quantized = zâ‚‘ + stopgrad(quantized - zâ‚‘)
    xÌ‚ = model.decoder(quantized)  
    
    ğ“› = Flux.Losses.mse(x, xÌ‚) + ğ“›â‚—â‚â‚œâ‚‘â‚™â‚œ 
end