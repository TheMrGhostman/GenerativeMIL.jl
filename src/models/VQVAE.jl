stopgrad(x) = x
Zygote.@nograd stopgrad

abstract type Quantizer end

struct VectorQuantizer <: Quantizer 
    embedding
end


struct VectorQuantizerEMA <: Quantizer 
    embedding
    nâ‚‘â‚˜â‚
    mâ‚‘â‚˜â‚
end

struct VQVAE
    encoder
    latent_embedding
    decoder
    nâ‚‘â‚˜â‚::Union{AbstractArray{<:Real, 2}, Nothing} # not needed for gradient version
    mâ‚‘â‚˜â‚::Union{AbstractArray{<:Real, 2}, Nothing} # not needed for gradient version
end

Flux.@functor VQVAE

Flux.trainable(model::VQVAE) = (model.encoder, model.latent_embedding, model.decoder)

function vector_quantization(model::VQVAE, z::AbstractArray{<:Real})
    # flatten input zâ‚‘ #d, n, bs = size(zâ‚‘)
    size_ = size(z)
    d, n_elements = size_[1], prod(size_[2:end]) # now it works for nD array

    zâ‚‘ = reshape(z, d, n_elements)
    # Compute distance (e - x)Â² = eÂ² - 2ex + xÂ²
    dist = (sum(zâ‚‘ .^2, dims=1) 
            .+ sum(model.latent_embedding .^2, dims=1)' 
            .- (2 .* model.latent_embedding' * zâ‚‘ ))

    # Encoding
    encoding_indices = argmin(dist, dims=1)
    encodings = zeros_like(zâ‚‘, (size(model.latent_embedding, 2), n_elements)) # there has to be transposition or dropdims
    Zygote.ignore(()->(encodings[encoding_indices] .+= 1)) # need to exclude gradient or i will crash

    # Quantize and unflatten
    quantized = model.latent_embedding * encodings # (d, e) * (e, bs) -> (d, bs)
    quantized = reshape(quantized, size_)
    return quantized, encodings
end

function (model::VQVAE)(x::AbstractArray{T}) where T<:Real
    zâ‚‘ =  model.encoder(x)
    quantized, _ = vector_quantization(model, zâ‚‘)
    quantized = zâ‚‘ + stopgrad(quantized - zâ‚‘) # gradient bypass
    xÌ‚ = model.decoder(quantized)  
    return xÌ‚
end


function loss_gradient(model::VQVAE, x::AbstractArray{T}; Î²::T=T(1)) where T<:Real
    zâ‚‘ =  model.encoder(x)
    quantized, _ = vector_quantization(model, zâ‚‘)
    
    # Embedding loss (e -> zâ‚‘) + Commitment loss (zâ‚‘ -> e)
    e_latent_loss = Flux.Losses.mse(stopgrad(quantized), zâ‚‘) # gradient propagation to encoder
    q_latent_loss = Flux.Losses.mse(quantized, stopgrad(zâ‚‘)) # gradient propagation to embedding

    ğ“›â‚—â‚â‚œâ‚‘â‚™â‚œ = q_latent_loss + Î² * e_latent_loss

    # Bypass of gradients from decoder to encoder 
    quantized = zâ‚‘ + stopgrad(quantized - zâ‚‘)

    xÌ‚ = model.decoder(quantized)  
    
    ğ“› = Flux.Losses.mse(x, xÌ‚) + ğ“›â‚—â‚â‚œâ‚‘â‚™â‚œ # in paper was mse(x, xÌ‚) / data_variance
end

function loss_ema(model::VQVAE, x::AbstractArray{T}; Î³::T=T(0.99), Ïµ::T=T(1e-5), trainmode::Bool=true) where T<:Real
    zâ‚‘ =  model.encoder(x)
    quantized, encodings = vector_quantization(model, zâ‚‘)
    # Commitment loss (zâ‚‘ -> embedding)
    e_latent_loss = Flux.Losses.mse(stopgrad(quantized), zâ‚‘) # gradient propagation to encoder
    ğ“›â‚—â‚â‚œâ‚‘â‚™â‚œ = Î² * e_latent_loss

    if trainmode 
        if isempty(model.ema.N)
            model.ema.N = 0
            model.ema.m = randn_like(model.latent_embedding)
        end
        # Exponential Moving Average update
        ema_cs = ema_cs .* Î³ .+ (1f0 - Î³) .* sum(encodings, dims=2) # (e, 1)
        n = sum(ema_cs) # scalar
        ema_cs = (ema_cs .+ Ïµ) ./ (n .+ size(model.latent_embedding, 2) .* Ïµ) .* n #(e, 1)
        d = size(zâ‚‘, 1) 
        dw = reshape(zâ‚‘, d, :) * encodings' # (d, n*bs) * (n*bs, e) -> (d, e) â‰ˆ latent_embedding
        ema_w = ema_w .* Î³ .+ (1f0 - Î³) .* dw # (d, e)
        model.latent_embedding .= ema_w ./ ema_cs # (d, e)
    end

    # Bypass of gradients from decoder to encoder 
    quantized = zâ‚‘ + stopgrad(quantized - zâ‚‘)

    xÌ‚ = model.decoder(quantized)  
    
    ğ“› = Flux.Losses.mse(x, xÌ‚) + ğ“›â‚—â‚â‚œâ‚‘â‚™â‚œ 
end
