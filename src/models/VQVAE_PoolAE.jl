stopgrad(x) = x
Zygote.@nograd stopgrad

function (q::Quantizer)(z::AbstractArray{<:Real}, y::T) where T<:Real # supervised forward pass
    # flatten input z‚Çë #d, n, bs = size(z‚Çë)
    size_ = size(z)
    d, n_elements = size_[1], prod(size_[2:end]) # now it works for nD array

    z‚Çë = reshape(z, d, n_elements)
    # Compute distance (e - x)¬≤ = e¬≤ - 2ex + x¬≤
    dist = (sum(z‚Çë .^2, dims=1) 
            .+ sum(q.embedding .^2, dims=1)' 
            .- (2 .* q.embedding' * z‚Çë ))
    
    # Supervised encodings
    close_indices = Zygote.ignore(()->(Matrix([CartesianIndex(y, 1);;])))
    close_encodings = zeros_like(z‚Çë, (size(q.embedding, 2), n_elements))
    Zygote.ignore(()->(close_encodings[close_indices] .+= 1)) # need to exclude gradient or it will crash

    # Encoding # unsupervised version
    encoding_indices = argmin(dist, dims=1)
    encodings = zeros_like(z‚Çë, (size(q.embedding, 2), n_elements)) # there has to be transposition or dropdims
    Zygote.ignore(()->(encodings[encoding_indices] .+= 1)) # need to exclude gradient or it will crash

    indicator = 1 .- (encoding_indices .== close_indices)[:]

    # Quantize and unflatten
    quantized = q.embedding * encodings # (d, e) * (e, bs) -> (d, bs)
    quantized = reshape(quantized, size_)

    close_quantized = q.embedding * close_encodings
    close_quantized = reshape(close_quantized, size_)

    return quantized, close_quantized, indicator
end

struct VQ_PoolAE
    encoder::PoolEncoder
    quantizer::Quantizer
    generator
    decoder
end

Flux.@functor VQ_PoolAE

function (model::VQ_PoolAE)(x::AbstractArray{T,2}) where T<:Real
    d, n = size(x) 
    z‚Çë =  model.encoder(x) # (d, n) -> (dz, 1)
    quantized, _ = model.quantizer(z‚Çë) # (dz, 1) -> (dz, 1)
    quantized = z‚Çë + stopgrad(quantized - z‚Çë) # gradient bypass
    Œº, Œ£ = model.generator(quantized)  # (dz, 1) -> ((dd, 1), (dd, 1))
    z‚Çõ = Œº .+ Œ£ .* randn_like(Œº, (size(Œº, 1), n)) # ((dd, 1), (dd, 1)) -> (dd, n)
    xÃÇ = model.decoder(z‚Çõ)  # (dd, n) -> (d, n)
    return xÃÇ
end

function loss_gradient(model::VQ_PoolAE, x::AbstractArray{T,2}; Œ≤=0.25) where T<:Real
    d, n = size(x) 
    z‚Çë =  model.encoder(x)
    quantized, _ = model.quantizer(z‚Çë)
    
    # Embedding loss (e -> z‚Çë) + Commitment loss (z‚Çë -> e)
    e_latent_loss = Flux.Losses.mse(stopgrad(quantized), z‚Çë) # gradient propagation to encoder
    q_latent_loss = Flux.Losses.mse(quantized, stopgrad(z‚Çë)) # gradient propagation to embedding

    ùìõ‚Çó‚Çê‚Çú‚Çë‚Çô‚Çú = q_latent_loss + Œ≤ * e_latent_loss

    # Bypass of gradients from decoder to encoder 
    quantized = z‚Çë + stopgrad(quantized - z‚Çë)

    Œº, Œ£ = model.generator(quantized)  # (dz, 1) -> ((dd, 1), (dd, 1))
    z‚Çõ = Œº .+ Œ£ .* randn_like(Œº, (size(Œº, 1), n)) # ((dd, 1), (dd, 1)) -> (dd, n)

    xÃÇ = model.decoder(z‚Çõ)  
    ùìõ = Flux3D.chamfer_distance(x, xÃÇ) + ùìõ‚Çó‚Çê‚Çú‚Çë‚Çô‚Çú # in paper was mse(x, xÃÇ) / data_variance
end


function loss_gradient(model::VQ_PoolAE, x::AbstractArray{<:Real,2}, y::Real; Œ≤=0.25, Œ≥=0.1)
    d, n = size(x) 
    z‚Çë =  model.encoder(x)
    pred_quantized, true_quantized, indicator = model.quantizer(z‚Çë, y)
    
    # Embedding loss (e -> z‚Çë) + Commitment loss (z‚Çë -> e)
    e_latent_loss = Flux.Losses.mse(stopgrad(true_quantized), z‚Çë) # gradient propagation to encoder
    q_latent_loss = Flux.Losses.mse(true_quantized, stopgrad(z‚Çë)) # gradient propagation to embedding

    d_latent_loss = indicator[1] .* Flux.Losses.mse(stopgrad(pred_quantized), z‚Çë) # gradient propagation to encoder
    x_latent_loss = indicator[1] .* Flux.Losses.mse(pred_quantized, stopgrad(z‚Çë)) # gradient propagation to embedding


    ùìõ‚Çó‚Çê‚Çú‚Çë‚Çô‚Çú = q_latent_loss + Œ≤ * e_latent_loss - x_latent_loss - Œ≥ * d_latent_loss

    # Bypass of gradients from decoder to encoder 
    quantized = z‚Çë + stopgrad(true_quantized - z‚Çë)

    Œº, Œ£ = model.generator(quantized)  # (dz, 1) -> ((dd, 1), (dd, 1))
    z‚Çõ = Œº .+ Œ£ .* randn_like(Œº, (size(Œº, 1), n)) # ((dd, 1), (dd, 1)) -> (dd, n)

    xÃÇ = model.decoder(z‚Çõ)  
    ùìõ = Flux3D.chamfer_distance(x, xÃÇ) + ùìõ‚Çó‚Çê‚Çú‚Çë‚Çô‚Çú # in paper was mse(x, xÃÇ) / data_variance
end



function loss_ema(model::VQ_PoolAE, x::AbstractArray{T,2}; Œ≤=0.25f0, Œ≥::T=T(0.99), œµ::T=T(1e-5), trainmode::Bool=true) where T<:Real
    d, n = size(x) 
    z‚Çë =  model.encoder(x)
    quantized, encodings = model.quantizer(z‚Çë)
    # Commitment loss (z‚Çë -> embedding)
    e_latent_loss = Flux.Losses.mse(stopgrad(quantized), z‚Çë) # gradient propagation to encoder
    ùìõ‚Çó‚Çê‚Çú‚Çë‚Çô‚Çú = Œ≤ * e_latent_loss

    if trainmode 
        # Exponential Moving Average update
        Zygote.ignore(()->ema_update!(model.quantizer, z‚Çë, encodings; Œ≥=Œ≥, œµ=œµ))
        # Zygote.ignore is probably not needed but .... just to be sure
    end

    # Bypass of gradients from decoder to encoder 
    quantized = z‚Çë + stopgrad(quantized - z‚Çë)

    Œº, Œ£ = model.generator(quantized)  # (dz, 1) -> ((dd, 1), (dd, 1))
    z‚Çõ = Œº .+ Œ£ .* randn_like(Œº, (size(Œº, 1), n)) # ((dd, 1), (dd, 1)) -> (dd, n)

    xÃÇ = model.decoder(z‚Çõ)  
    
    ùìõ = Flux3D.chamfer_distance(x, xÃÇ) + ùìõ‚Çó‚Çê‚Çú‚Çë‚Çô‚Çú 
end

function vq_poolae_constructor_from_named_tuple(;idim=3, prpdim=64, prpdepth=3, popdim=128, popdepth=3, zdim=32, 
    decdim=64, decdepth=3, poolf="mean-max", gen_sigma="scalar", n_embed=128, ema=false, activation::String="swish", 
    init_seed=nothing, kwargs...)
    """
    -----------------------
    VQ_PoolAE constructor
    -----------------------
    idim        -> input dimensions
    prpdim      -> hidden dimension for Pre Pooling part (PreP)
    prpdepth    -> number of layers in PreP
    popdim      -> hidden dimension for Post Pooling part (PostP)
    popdepth    -> number of layers in PostP
    zdim        -> dimension of latent space
    decdim      -> hidden dimension for decoder part
    decdepth    -> number of layers in decoder
    poolf       -> pooling function (\"mean-max\", \"mean\", \"max\", \"attention\", \"PMA\")
    gen_sigma   -> type of variance in generator (\"scalar\" or \"diag\")
    activation  -> activation function 
    """

    activation = eval(:($(Symbol(activation))))
    (init_seed !== nothing) ? Random.seed!(init_seed) : nothing
    if gen_sigma == "scalar"
        gen_out_dim = 1
    elseif gen_sigma == "diag"
        gen_out_dim = zdim
    else
        error("Unkown type of vairance")
    end

    prepool = Flux.Chain(
        Flux.Dense(idim, prpdim, activation), 
        [Flux.Dense(prpdim, prpdim, activation) for i=1:prpdepth-1]...
    )

    multiplier=1
    if poolf=="mean-max"
        fpool = x->cat(mean(x, dims=2), maximum(x, dims=2), dims=1)
        multiplier = 2
    elseif poolf=="mean"
        fpool = x->mean(x, dims=2)
    elseif poolf=="max"
        fpool = x->maximum(x, dims=2)
    elseif poolf=="attention"
        fpool = AttentionPooling(Flux.Chain(
                Dense(prpdim, prpdim, activation),
                Dense(prpdim,1)
                ))
    elseif poolf=="PMA"
        fpool = PMA(1, prpdim, 4)
    else
        error("Unknown pooling function")
    end

    postpool = Flux.Chain(
        Flux.Dense(multiplier*prpdim, popdim, activation), 
        [Flux.Dense(popdim, popdim, activation) for i=1:popdepth-1]...
    )

    decoder = Flux.Chain(
        Flux.Dense(zdim, decdim, activation), 
        [Flux.Dense(decdim, decdim, activation) for i=1:decdepth-2]...,
        Flux.Dense(decdim, idim), 
    )
    if ema
        emb = randn(Float32, popdim, n_embed) 
        quan = VectorQuantizerEMA(emb, zeros(Float32, n_embed, 1), rand_like(emb))
    else
        emb = randn(Float32, popdim, n_embed) 
        #emb = Float32.(rand(Uniform(-1/n_embed, 1/n_embed), zdim, n_embed))
        quan = VectorQuantizer(emb)
    end

    encoder = PoolEncoder(prepool, fpool, postpool)
    generator = SplitLayer(popdim, (zdim, gen_out_dim), (identity, Flux.softplus))
    (init_seed !== nothing) ? Random.seed!() : nothing
    return VQ_PoolAE(encoder, quan, generator, decoder)
end





struct VectorGaussianQuantizerEMA <: Quantizer 
    embedding_mean::AbstractArray{<:Real, 2}
    embedding_std::AbstractArray{<:Real, 2}
    n::AbstractArray{<:Real, 2}
    m::AbstractArray{<:Real, 2}
    s::AbstractArray{<:Real, 2}
end

Flux.@functor VectorGaussianQuantizerEMA
Flux.trainable(q::VectorGaussianQuantizerEMA) = ()#q.embedding_mean, q.embedding_std, 
# no tranable of ema

function (q::VectorGaussianQuantizerEMA)(z::AbstractArray{<:Real,2})
    # flatten input z‚Çë #d, n, bs = size(z‚Çë)
    size_ = size(z)
    d, n_elements = size_[1], prod(size_[2:end]) # now it works for nD array

    z‚Çë = reshape(z, d, n_elements)

    ss = (q.embedding_mean .- z‚Çë).^2
    norm_const = 0.5 .*log.(prod(q.embedding_std, dims=1))
    dist = (Flux.sum(-0.5 .*(log(2*pi) .+ ss./q.embedding_std), dims=1) .- norm_const)'
    # Flux.sum(-0.5 .*(log(2*pi) .+ ((Œº_q .- z‚Çë).^2)./Œ£_q), dims=1) .- 0.5 .*log.(prod(Œ£_q, dims=1))
    
    # Encoding
    encoding_indices = argmin(dist, dims=1)
    encodings = zeros_like(z‚Çë, (size(q.embedding_mean, 2), n_elements)) # there has to be transposition or dropdims
    Zygote.ignore(()->(encodings[encoding_indices] .+= 1)) # need to exclude gradient or it will crash

    # Quantize and unflatten
    quantized_mean = q.embedding_mean * encodings # (d, e) * (e, 1) -> (d, 1)
    quantized_std = q.embedding_std * encodings # (d, e) * (e, 1) -> (d, 1)
    return reshape(quantized_mean, size_), reshape(quantized_std, size_), encodings

end

function ema_update!(q::VectorGaussianQuantizerEMA, z, encodings; Œ≥=0.99f0, œµ=1f-5) 
    q.n .= q.n .* Œ≥ .+ (1f0 - Œ≥) .* sum(encodings, dims=2) # (e, 1)
    N = sum(q.n)
    q.n .= (q.n .+ œµ) ./ (N .+ size(q.embedding_mean, 2) .* œµ) .* N # (e, 1)

    d = size(z, 1)
    dw = reshape(z, d, :) * encodings' # (d, n_elements) x (n_elements, e) -> (d, e)

    q.m .= q.m .* Œ≥ .+ (1f0 - Œ≥) .* dw # (d, e)
    q.s .= q.s .* Œ≥ .+ (1f0 - Œ≥) .* (dw .- q.m).^2    # dw is sum but only for n>1
    # FIXME for gpu vectorized training this (q.s) needs to be fixed
    q.embedding_mean .= q.m ./ q.n' # (d, e) .* (1, e) -> (d, e)
    q.embedding_std .= sqrt.(q.s ./ q.n') # (d, e) .* (1, e) -> (d, e)
end


struct VGQ_PoolAE
    encoder::PoolEncoder
    quantizer::VectorGaussianQuantizerEMA
    decoder
end

Flux.@functor VGQ_PoolAE

function (model::VGQ_PoolAE)(x::AbstractArray{<:Real, 2})
    d, n = size(x) # (d, n)
    z‚Çë =  model.encoder(x) # (d, n) -> (zdim, 1) | PoolEncoder
    Œº_q, Œ£_q, encodings = model.quantizer(z‚Çë) # (zdim, 1) -> ((zdim, 1), (zdim, 1), (zdim, e))

    quantized = Œº_q .+ Œ£_q .* randn_like(z‚Çë, (size(Œº_q,1), n)) # ((zdim, 1) .+ (zdim, 1) .* (zdim, n)) -> (zdim, n)
    quantized = z‚Çë .+ stopgrad(quantized .- z‚Çë) # ((zdim, 1) .+ (zdim, n) .- (zdim, 1)) -> (zdim, n)
    xÃÇ = model.decoder(quantized)  # (zdim, n) -> (d, n)
end

function loss_ema(model::VGQ_PoolAE, x::AbstractArray{T,2}; Œ≤=0.25f0, Œ≥::T=T(0.99), œµ::T=T(1e-5), trainmode::Bool=true) where T<:Real
    d, n = size(x) # (d, n)
    z‚Çë =  model.encoder(x) # (d, n) -> (zdim, 1) | PoolEncoder
    Œº_q, Œ£_q, encodings = model.quantizer(z‚Çë) # (zdim, 1) -> ((zdim, 1), (zdim, 1), (zdim, e))
    # Commitment loss (z‚Çë -> embedding)
    e_latent_loss = (Flux.sum(-0.5 .*(log(2*pi) .+ ((Œº_q .- z‚Çë).^2)./Œ£_q)) - 0.5*log(prod(Œ£_q))) # scalar
    # equivalent to -logpdf(MultvariateNormal(Œº_q, Œ£_q), z‚Çë)
    ùìõ‚Çó‚Çê‚Çú‚Çë‚Çô‚Çú = Œ≤ * e_latent_loss / n # scale loss function

    if trainmode 
        # Exponential Moving Average update
        Zygote.ignore(()->ema_update!(model.quantizer, z‚Çë, encodings; Œ≥=Œ≥, œµ=œµ)) 
    end

    # Bypass of gradients from decoder to encoder 
    quantized = Œº_q .+ Œ£_q .* randn_like(z‚Çë, (size(Œº_q,1), n)) # ((zdim, 1) .+ (zdim, 1) .* (zdim, n)) -> (zdim, n)
    quantized = z‚Çë .+ stopgrad(quantized .- z‚Çë) # ((zdim, 1) .+ (zdim, n) .- (zdim, 1)) -> (zdim, n)
    xÃÇ = model.decoder(quantized)  # (zdim, n) -> (d, n)
    ùìõ = Flux3D.chamfer_distance(x, xÃÇ) + ùìõ‚Çó‚Çê‚Çú‚Çë‚Çô‚Çú 
end

function vgq_poolae_constructor_from_named_tuple(
    ;idim=3, prpdim=64, prpdepth=3, popdim=64, popdepth=3, zdim=3, n_embed=128, 
    decdim=64, decdepth=3, poolf="mean-max", activation="swish", init_seed=nothing, kwargs...)
    
    activation = eval(:($(Symbol(activation))))
    (init_seed !== nothing) ? Random.seed!(init_seed) : nothing

    prepool = Flux.Chain(
        Flux.Dense(idim, prpdim, activation), 
        [Flux.Dense(prpdim, prpdim, activation) for i=1:prpdepth-1]...
    )

    multiplier=1
    if poolf=="mean-max"
        fpool = x->cat(mean(x, dims=2), maximum(x, dims=2), dims=1)
        multiplier = 2
    elseif poolf=="mean"
        fpool = x->mean(x, dims=2)
    elseif poolf=="max"
        fpool = x->maximum(x, dims=2)
    elseif poolf=="attention"
        fpool = AttentionPooling(Flux.Chain(
                Dense(prpdim, prpdim, activation),
                Dense(prpdim,1)
                ))
    elseif poolf=="PMA"
        fpool = PMA(1, prpdim, 4)
    else
        error("Unknown pooling function")
    end

    postpool = Flux.Chain(
        Flux.Dense(multiplier*prpdim, popdim, activation), 
        [Flux.Dense(popdim, popdim, activation) for i=1:popdepth-2]...,
        Flux.Dense(popdim, zdim)
    )
    
    decoder = Flux.Chain(
        Flux.Dense(zdim, decdim, activation), 
        [Flux.Dense(decdim, decdim, activation) for i=1:decdepth-2]...,
        Flux.Dense(decdim, idim), 
    )

    emb_m = sample_sphere(zdim, n_embed)
    emb_s = ones(Float32, zdim, n_embed);
    quan = VectorGaussianQuantizerEMA(
        emb_m, 
        emb_s, 
        zeros(Float32, n_embed, 1), 
        rand_like(emb_m), 
        ones_like(emb_m)
    );

    encoder = PoolEncoder(prepool, fpool, postpool)
    model = VGQ_PoolAE(encoder, quan, decoder)

    (init_seed !== nothing) ? Random.seed!() : nothing
    return model
end





function Base.show(io::IO, m::VectorGaussianQuantizerEMA)
    print(io, "VectorGaussianQuantizerEMA")
    print(io, "\n- embedding_mean = $(m.embedding_mean |> size) | $(m.embedding_mean  |> typeof)\
    | mean ~ $(m.embedding_mean  |> Flux.mean)")
    print(io, "\n- embedding_std = $(m.embedding_std |> size) | $(m.embedding_std |> typeof) \
    | mean ~ $(m.embedding_std |> Flux.mean)")
    print(io, "\n- n·µó·µ¢ = $(m.n |> size) | $(m.n  |> typeof) | mean ~ $(m.n  |> Flux.mean)")
    print(io, "\n- m·µó·µ¢ = $(m.m |> size) | $(m.m  |> typeof) | mean ~ $(m.m  |> Flux.mean)")
    print(io, "\n- s·µó·µ¢ = $(m.s |> size) | $(m.s  |> typeof) | mean ~ $(m.s  |> Flux.mean)")
end